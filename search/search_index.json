{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Search-a-licious","text":"<p>Welcome to the documentation of search-a-licious, a ready to use search component built on top of Elasticsearch.</p>"},{"location":"#executive-summary","title":"Executive summary","text":"<p>Search-a-licious transforms large data collections into searchable content, allowing users to quickly find what they need with powerful queries, facet exploration, and visualizations. Originally developed for Open Food Facts, it\u2019s perfect for exposing data, supporting decision-making, and enabling exploration, unlocking the full potential of open data collections. Developers can rapidly build apps thanks to its easy configuration, reusable components, and robust architecture.</p>"},{"location":"#what-is-search-a-licious","title":"What is search-a-licious ?","text":"<p>We created this project in order to leverage collections of (uniform enough) data to build tools that can:</p> <ul> <li>make this data reachable by a large public</li> <li>help users find the right items according to their criteria</li> <li>enable data exploration and get useful insights</li> </ul> <p>It provides a ready to use component to:</p> <ul> <li>provide an engaging search experience with<ul> <li>a fast full text search</li> <li>facets to refine results</li> <li>as you type filter suggestions</li> </ul> </li> <li>personalize results to specific needs thanks to specific sorting</li> <li>help your users build insights with instant charts</li> <li>build powerful in app features thanks to a powerful API</li> </ul>"},{"location":"#technical-overview","title":"Technical overview","text":"<p>On a technical level, you can use:</p> <ul> <li>web components to quickly build your UI using any JavaScript framework, or plain HTML</li> <li>sensible defaults to provide a good search experience</li> <li>an easy to setup, one stop, file configuration to describe your content</li> <li>a ready to deploy Docker Compose file including all needed services</li> <li>a one command initial data import from a JSONL data export</li> <li>continuous update through a stream of events</li> </ul> <p>It leverages existing components:</p> <ul> <li>Elasticsearch for the search engine</li> <li>Web Components (built thanks to Lit framework)</li> <li>Vega for the charts</li> <li>Redis for event stream</li> </ul> <p>Read our tutorial to get started !</p>"},{"location":"#contributing","title":"Contributing","text":"<p>This is an Open Source project and contributions are really welcome !</p> <p>See our developer introduction to get started</p> <p>Every contribution as bug report, documentation, UX design is also really welcome ! See our wiki page about Open Food Facts</p>"},{"location":"#documentation-organization","title":"documentation organization","text":"<p>The documentation is split between User documentation (for re-users, third party developers) and Developer documentation (for contributors).</p> <p>The documentation follows the Di\u00e1taxis framework.</p> <p>Pages title should start with:</p> <ul> <li>tutorial on - tutorials aimed at learning</li> <li>how to\u2026 - how to guides to reach a specific goal</li> <li>explain\u2026 - explanation to understand a topic</li> <li>reference\u2026 - providing detailed information</li> </ul>"},{"location":"#thank-you-to-our-sponsors","title":"Thank you to our sponsors !","text":"<p>This project has received financial support from the NGI Search (New Generation Internet) program, funded by the \ud83c\uddea\ud83c\uddfa European Commission. Thank you for supporting Open-Source, Open Data, and the Commons.</p> <p> </p>"},{"location":"devs/explain-architecture/","title":"Explain Architecture","text":"<p>Refer to the introduction to get main components.</p> <p>This image illustrates the main components and their interactions.</p> <p> (source)</p> <p>The main idea is to have:</p> <ul> <li>a very good stack with best in class open source components (see docker compose files)</li> <li>a very powerful API that is capable and easy to use.   This is achieved through the use of Lucene Query Language and abstracting some complex processing under the hood (synonyms, language support, aggregations, etc.)</li> <li>a single, easy to use configuration file to declare your data structure and what you need to search</li> <li>web components to quickly build UI's while still being able to customize it in many scenarios.</li> </ul>"},{"location":"devs/explain-architecture/#archive","title":"Archive","text":"<p>The initial Search-a-licious roadmap architecture notes might be interesting to grasp the general idea of the project architecture.</p>"},{"location":"devs/explain-web-frontend/","title":"Explain web frontend","text":"<p>The search-a-licious web frontend supplies web components. These are built using lit and typescript.</p> <p>You can find the documentation for each widget in the Reference for Search-a-licious Web Components file.</p>"},{"location":"devs/explain-web-frontend/#explanation-on-code-structure","title":"Explanation on code structure","text":"<p>We use web-components for they will enable integration in a very wide variety of situations.</p> <p>The <code>search-ctl.ts</code> file contains the search controller, which is responsible for launching search and dispatching results. In practice this is a mixin, used by the search bar components which gets this role of controller. It is the contact point with the API. The controller have a specific name, and components that are linked to it refer this the search bar name (<code>search-name</code> property). The default is 'searchalicious'.</p> <p>Components communicate with the search controller thanks to events, see <code>events.ts</code>. There is an event to launch a search or change page, and one to dispatch search results. Events always contains the search name, so we could have more than one search on the same page.</p> <p>We tend to factor code when it make sense using mixins, for example as there are lots of component that needs the search results, there is a mixin than contains the logic to register to such events (see <code>search-results-ctl.ts</code>).</p>"},{"location":"devs/explain-web-frontend/#writing-documentation","title":"Writing documentation","text":"<p>We render the reference on web components using <code>api-viewer</code> web component.</p> <p>Please comply with JSDoc and document every property / slots etc. on each web components.</p>"},{"location":"devs/explain-web-frontend/#tools","title":"Tools","text":"<p>Thanks to Makefile in root folder,</p> <ul> <li><code>make check_front</code> run all checks in front</li> <li><code>make lint_front</code> lint js code</li> </ul> <p>While coding, you might want to run: <code>make tsc_watch</code> to have your code compile every time you save a <code>.ts</code> file.</p> <p>We generate a custom-elements.json manifest using custom elements manifest analyzer. Please use supported JSDoc markers in your code to document components.</p> <p>The components documentation is rendered in <code>web-components.html</code>, using the api-viewer component</p>"},{"location":"devs/explain-web-frontend/#tests","title":"Tests","text":"<p><code>make test_front</code> run js tests.</p> <p>Note that we use:</p> <ul> <li>Open Web Component testing framework,   which in turn uses:<ul> <li>Mocha as the test runner</li> <li>which runs tests using playwright</li> <li>and Chai for assertions</li> </ul> </li> </ul>"},{"location":"devs/explain-web-frontend/#translations","title":"Translations","text":"<p>In the frontend, we utilize lit-localize, a library that leverages lit-element for managing translations from hardcoded text. The language is set to the browser's language if it is supported by the project, otherwise it is set to default language (English). The translations are stored in <code>xliff</code> files in the <code>frontend/xliff</code> directory.</p> <p>To add a new translation you need to :</p> <ul> <li>add <code>msg</code> in your code like this https://lit.dev/docs/localization/overview/#message-types</li> <li>run <code>npm run translations:extract</code> to extract the new translations</li> <li>add your translation with 'target' tag in the <code>xliff/&lt;your_language&gt;.xlf</code> files</li> <li>run <code>npm run translations:build</code> to update the translations in the <code>src/generated/locales/&lt;your_language&gt;.js</code> file</li> </ul> <p>To add a language, you have to add the language code to <code>targetLocales</code> in <code>lit-localize.json</code></p>"},{"location":"devs/explain-web-frontend/#translations-in-crowdin","title":"Translations in Crowdin","text":"<p>We can use Crowdin to manage translations. All files in the xliff/ folder can be uploaded to Crowdin, as it supports the xlf format.</p>"},{"location":"devs/how-to-debug-backend/","title":"How to Debug the backend app","text":"<p>By default, the API runs on uvicorn which use autoreloading and more than one thread, also it does not have a tty. So if you use <code>pdb.set_trace()</code> you won't be able to access the console.</p> <p>To debug the backend app:</p> <ul> <li>stop API instance: <code>docker compose stop api</code></li> <li>add a pdb.set_trace() at the point you want,</li> <li>then launch <code>docker compose run --rm  --use-aliases api uvicorn app.api:app --proxy-headers --host 0.0.0.0 --port 8000 --reload</code>[^use_aliases]</li> <li>go to the url you want to test</li> </ul>"},{"location":"devs/how-to-install/","title":"How to install for local development","text":""},{"location":"devs/how-to-install/#option-1-dev-containers-recommended","title":"Option 1: Dev Containers (Recommended)","text":"<p>The easiest way to get started is using Dev Containers, which provides a fully-configured development environment.</p>"},{"location":"devs/how-to-install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and running</li> <li>VS Code with the Dev Containers extension</li> <li>Or use GitHub Codespaces for browser-based development</li> </ul>"},{"location":"devs/how-to-install/#getting-started","title":"Getting Started","text":"<ol> <li>Clone the repository</li> <li>Open the folder in VS Code</li> <li>When prompted, click \"Reopen in Container\" (or use the command palette: <code>Dev Containers: Reopen in Container</code>)</li> <li>Wait for the container to build (first time takes a few minutes)</li> <li>You're ready to develop! All dependencies are pre-installed.</li> </ol> <p>The dev container automatically:</p> <ul> <li>Sets up Python 3.11 with Poetry and all backend dependencies</li> <li>Installs Node.js LTS with npm and all frontend dependencies</li> <li>Configures Docker-in-Docker for running compose commands</li> <li>Installs helpful VS Code extensions</li> <li>Starts Elasticsearch, Redis, and other services</li> <li>Sets up pre-commit hooks</li> </ul> <p>You can now use all <code>make</code> commands and start developing immediately. Skip to the Importing data section to load sample data.</p>"},{"location":"devs/how-to-install/#option-2-manual-installation","title":"Option 2: Manual Installation","text":""},{"location":"devs/how-to-install/#pre-requisites","title":"Pre-requisites","text":"<p>First, follow same prerequisite as for normal installation:</p> <ul> <li>configuring mmap count</li> <li>installing docker and docker compose</li> </ul>"},{"location":"devs/how-to-install/#installing-pre-commit","title":"Installing Pre-commit","text":"<p>We use pre-commit to check the code quality.</p> <p>You can follow the following tutorial to install pre-commit on your machine.</p>"},{"location":"devs/how-to-install/#auto-fixing-linting-issues-in-pull-requests","title":"Auto-fixing linting issues in Pull Requests","text":"<p>For Pull Requests, you can automatically fix linting issues by commenting <code>/fix-linting</code> on the PR. This will trigger a GitHub Action that runs the linting tools and commits any fixes directly to the PR branch.</p>"},{"location":"devs/how-to-install/#installing-direnv","title":"Installing Direnv","text":"<p>Direnv is a tool to automatically set environment variables depending on the current directory. This is handy to personalize the environment for each project as environments variables have priority over the <code>.env</code> file.</p> <p>For Linux and macOS users, You can follow our tutorial to install direnv.<sup>1</sup></p>"},{"location":"devs/how-to-install/#setting-up-your-environment","title":"Setting up your environment","text":"<p>You have several options to set up your environment:</p> <ol> <li>use direnv, and thus use the <code>.envrc</code> file to set up your environment</li> <li>add a .envrc that you source in your terminal.</li> <li>modify the .env file directly, in which case you should be careful to not commit your changes</li> </ol> <p>The 1st and 2nd options are the recommended ones. The following steps are for those options, in case you edit the <code>.env</code> just ignore the \"export \" keywords.</p> <p>Get your user id and group id by running <code>id -u</code> and <code>id -g</code> in your terminal.</p> <p>Add a <code>.envrc</code> file at the root of the project with the following content:</p> <pre><code>export USER_GID=&lt;your_user_gid&gt;\nexport USER_UID=&lt;your_user_uid&gt;\n\nexport CONFIG_PATH=data/config/openfoodfacts.yml\nexport OFF_API_URL=https://world.openfoodfacts.org\n</code></pre>"},{"location":"devs/how-to-install/#building-containers","title":"Building containers","text":"<p>To build the containers, you can run the following command:</p> <pre><code>make build\n</code></pre> <p>Note: the Makefile will align the user id with your own uid for a smooth editing experience (having same user id in container and host, so that you have permission to edit files).</p>"},{"location":"devs/how-to-install/#running","title":"Running","text":"<p>Now you can run the project with Docker <code>docker compose up</code>.</p> <p>After that run the following command on another shell to compile the project: <code>make tsc_watch</code>.</p> <p>Do this for next installation steps and to run the project.</p> <p>[!NOTE]</p> <ul> <li>You may encounter a permission error if your user is not part of the <code>docker</code> group, in which case you should either add it or modify the Makefile to prefix <code>sudo</code> to all docker and docker compose commands.</li> <li>Update container crash because we are not connected to any Redis, this is not a problem</li> </ul> <p>Docker spins up:</p> <ul> <li>Two elasticsearch nodes, one being exposed on port 9200 <sup>2</sup><ul> <li>test it going to http://127.0.0.1:9200</li> </ul> </li> <li>Elasticvue on port 8080<ul> <li>test it going to http://127.0.0.1:8080</li> </ul> </li> <li>The search service on port 8000<ul> <li>test the API going to http://search.localhost:8000/docs</li> <li>test the UI going to http://search.localhost:8000/</li> </ul> </li> </ul> <p>Congratulations, you have successfully installed the project !</p> <p>You will then need to import from a JSONL dump (see instructions below).</p>"},{"location":"devs/how-to-install/#importing-data-into-your-development-environment","title":"Importing data into your development environment","text":"<ul> <li>Import Taxonomies: <code>make import-taxonomies</code></li> <li>Import products :   <pre><code> # get some sample data\n curl https://world.openfoodfacts.org/data/exports/products.random-modulo-10000.jsonl.gz --output data/products.random-modulo-10000.jsonl.gz\n gzip -d data/products.random-modulo-10000.jsonl.gz\n # we skip updates because we are not connected to any redis\n make import-dataset filepath='products.random-modulo-10000.jsonl' args='--skip-updates'\n</code></pre></li> </ul> <p>Verify you have data by going to http://search.localhost:8000/</p>"},{"location":"devs/how-to-install/#exploring-elasticsearch-data","title":"Exploring Elasticsearch data","text":"<p>When you need to explore the elasticsearch data, you can use elasticvue.</p> <ul> <li>Go to http://127.0.0.1:8080/welcome</li> <li>Click on \"Add Elasticsearch cluster\"</li> <li>change the cluster name to \"docker-cluster\" at http://127.0.0.1:9200</li> <li>Click on \"Connect\"</li> </ul> <ol> <li> <p>For Windows users, the .envrc is only taken into account by the <code>make</code> commands.\u00a0\u21a9</p> </li> <li> <p>by default we only expose on the localhost interface. This is driven by the <code>*_EXPOSE</code> variables in <code>.env</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"devs/introduction/","title":"Developer introduction","text":"<p>The Search-a-licious project is centered around a few main components:</p> <ul> <li>The API, using FastAPI, that you find in <code>app</code> folder. See it's ref documentation</li> <li>The web components to build your UI, using Lit that you find in <code>frontend</code> folder.   See Explain frontend</li> <li>Docker compose for deployment, see <code>docker-compose.yml</code> and <code>docker/</code> folder</li> </ul> <p>We use three main components:</p> <ul> <li>Elasticsearch for the search engine<sup>1</sup></li> <li>[Redis] for event stream<sup>2</sup></li> <li>Vega for the charts</li> </ul> <p>see Explain Architecture for more information.</p>"},{"location":"devs/introduction/#getting-started","title":"Getting started","text":"<p>See Install the project locally for development</p>"},{"location":"devs/introduction/#development-tips","title":"Development tips","text":"<ul> <li>How to debug the backend</li> </ul> <ol> <li> <p>Open Search is also a desirable target, contribution to verify compatibility and provide it as default would be appreciated.\u00a0\u21a9</p> </li> <li> <p>an alternative to Redis for event stream would also be a desirable target.\u00a0\u21a9</p> </li> </ol>"},{"location":"devs/ref-python/","title":"Ref: Python Code documentation \ud83e\udc55","text":"<p>Important: do not edit this file directly, it is autogenerated by <code>scripts/build_sphinx.sh</code> using <code>docs/sphinx</code> content.</p>"},{"location":"reports/2024-06%20thoughts%20on%20scoring/","title":"Thoughts on scoring","text":""},{"location":"reports/2024-06%20thoughts%20on%20scoring/#using-scripts","title":"Using scripts","text":"<p>This is the easiest method, and more generic one, but not necessarily optimize for big databases.</p> <ul> <li>scripts can be declared in the config</li> <li>we run a command to store them in Elasticsearch (removing the eventual non declared ones) ``</li> <li>(maybe at startup we should verify that the scripts declared in config are in ElasticSearch)</li> <li>scripts accepts parameters</li> <li>as you call search API, you specify a script name (as in config) and provide parameters</li> </ul> <p>It can be used for ranking and to compute specific attributes</p> <p>Pros:</p> <ul> <li>it can be secure as we provide scripts in the config</li> <li>it's a simple design, easy to implement</li> </ul> <p>Cons:</p> <ul> <li>performance might be an issue if there is no filters</li> </ul> <p>Filters might be a way to improve performances ?</p> <p>Performance might be better when people use vectors and vectors functions like dotProduct. This means supporting vectors in fields types.</p> <p>Having well designed filters to limit the score, might also help. Is there a way we put a maximum number of matching item before using a filter ?</p> <p>Also the script filter may help, if it's faster than score to remove some values (but is it worth it ?)</p>"},{"location":"reports/2024-06%20thoughts%20on%20scoring/#using-knn","title":"Using KNN","text":"<p>There is a good support for KNNs in ElasticSearch, with approximate matching.</p> <p>If we can reduce scoring to a KNN search this might be worth exploring.</p> <p>It can be combined with filters.</p> <p>But we might need to limit pagination as there is no way to really use pagination in this context. A filter might be used to remove first pages ids, but it is limited.</p>"},{"location":"reports/2024-06%20thoughts%20on%20scoring/#personal-search-at-open-food-facts","title":"Personal search at Open Food Facts","text":"<p>The score is a ponderated mean, so really it can be seen as a dotProduct</p> <p>We can put attributes match in a vector and put weighs of user attributes in another vector, then use dotProduct. ( a1 * b1 + a2 * b2 + a3 * b3)</p> <p>It is ok for ranking as \"maybe\" and \"unknown\" are not taken into account.</p> <p>The only problem is with \"non matching\" we should be less than 0 and is hard to express in a dotProduct (non matching is as soon as a mandatory is &lt; 10). This could be expressed by adding to the vector a special part where all attributes under 10 are 128 and 0 otherwise for the product, and for the parameter, we put -128 in the corresponding slot if attribute is mandatory.</p> <p>We can use byte vectors as values are between 0 and 100, which helps being fast. It also aleviate the constraint of having  score is 0.5 + (dot_product(query, vector) / (32768 * dims)), so to retrieve the dot product you should do (score - 0.5) * 32768 * dims</p>"},{"location":"reports/2024-06%20thoughts%20on%20scoring/#current-conclusions","title":"Current conclusions","text":"<ul> <li>start with implementing scripts score, in config, and enable their usage in the API</li> <li>enable having vector fields</li> <li>explore performance on this basis</li> <li>see if we can expose knn instead ?</li> <li>maybe expose available scripts in an api ?</li> </ul>"},{"location":"users/explain-configuration/","title":"Explain configuration file","text":"<p>The idea of Search-a-licious is that you provide all your configurations details in a central file, and all the rest works (at least for main scenarios).</p> <p>The configuration file is a YAML file.</p>"},{"location":"users/explain-configuration/#one-configuration-multiple-datasets","title":"One configuration, multiple datasets","text":"<p>A Search-a-licious instance only have one configuration file, but is capable of serving multiple datasets</p> <p>It provides a section for each index you want to create (corresponding to a dataset).</p> <p>If you have more than one dataset, one must be declared the default (see default_index)</p>"},{"location":"users/explain-configuration/#main-sections","title":"Main sections","text":"<p>For each indexe the main sections are:</p> <ul> <li>index: some configuration of the Elasticsearch index</li> <li>fields: the fields you want to put in the index, their type and other configurations</li> <li>taxonomy: definitions of taxonomies that are used by this index</li> <li>redis_stream_name and document_fetcher: if you use continuous updates, you will need to define one</li> <li>preprocessor and result_processor are two fields enabling to handle specificities of your dataset.</li> <li>scripts: to use sort by script (see How to use scripts)</li> </ul>"},{"location":"users/explain-configuration/#index-configuration","title":"Index configuration","text":"<p>Search-a-licious is really based upon Elasticsearch,</p> <p>This section provides some important fields to control the way it is used.</p> <p><code>id_field_name</code> is particularly important as it must contain a field that uniquely identifies each items. If you don't have such field, you might use <code>preprocessor</code> to compute one. It is important to have such an id to be able to use continuous updates.</p> <p><code>last_modified_field_name</code> is also important for continuous updates to decide where to start the event stream processing.</p>"},{"location":"users/explain-configuration/#fields","title":"Fields","text":"<p>This is one of the most important section.</p> <p>It specifies what will be stored in your index, which fields will be searchable, and how.</p> <p>You have to plan in advance how you configure this.</p> <p>Think well about:</p> <ul> <li>fields you want to search and how you want to search them</li> <li>which information you need to display in search results</li> <li>what you need to sort on</li> <li>which facets you want to display</li> <li>which charts you need to build</li> </ul> <p>Changing this section will probably involve a full re-indexing of all your items.</p> <p>Some typical configurations for fields:</p> <p>A tags field that as values that are searched as an exact value (aka keyword), eg. a tag: <pre><code>tags:\n    type: keyword\n</code></pre></p> <p>An ingredients field that is used for full text search when no field is specified: <pre><code>ingredients:\n    type: text\n    full_text_search: true\n</code></pre></p> <p>A field <code>product_name</code> that is used for full text search, but with multilingual support: <pre><code>product_name:\n    full_text_search: true\n    type: text_lang\n</code></pre></p> <p>A scans_n field is an integer field: <pre><code>scans_n:\n    type: integer\n</code></pre></p> <p>A <code>specific_warnings</code> field that is used for full text search, but only when you specify the field: <pre><code>specific_warnings:\n    type: text\n</code></pre></p> <p>A field brands_tags that needs to be split in multiple values (according to <code>split_separator</code> option): <pre><code>brands_tags:\n    type: keyword\n    split: true\n</code></pre></p> <p>A field labels_tags, that is used for exact match but with support of a taxonomy, and that can be used for faceting, and bar graph generation: <pre><code>labels_tags:\n    type: keyword\n    taxonomy_name: label\n    bucket_agg: true\n</code></pre></p> <p>Read more in the reference documentation.</p>"},{"location":"users/explain-configuration/#document-fetcher-pre-processors-and-post-processors","title":"Document fetcher, pre-processors and post-processors","text":"<p>It is not always straight forward to index an item.</p> <p>Search-a-licious offers a way for you to customize some critical operations using Python code.</p> <ul> <li>preprocessor adapts you document before being indexed</li> <li>whereas result_processor adapts each result returned by a search, keep it lightweight !</li> <li>document_fetcher is only used for continuous updates to fetch documents using an API</li> </ul> <p>Read more in the reference documentation.</p>"},{"location":"users/explain-configuration/#scripts","title":"Scripts","text":"<p>You can also add scripts for sorting documents. See How to use scripts.</p>"},{"location":"users/explain-query-language/","title":"Explain Query Language","text":"<p>The idea of Search-a-licious is to provide a powerfull yet easy to use API, through the use of a well proven language: Lucene Query Language.</p> <p>While Elasticsearch provides a way to use this language in the queries, it has some important limitations like the lack of support for nested and object fields.</p> <p>Thanks to the luqum library, Search-a-licious is able to use Lucene Query Language in a broader way.</p> <p>Search-a-licious also use luqum to introspect the query and transform it to add features corresponding to your configuration and leverage taxonomies and other peculiarities.</p> <p>It enables for example taking into account synonyms, or adding the languages to query about on the fly without the need to complexify the query to much at API level.</p>"},{"location":"users/explain-query-language/#query-syntax","title":"Query syntax","text":"<p>The query syntax is quite simple, you can either:</p> <ul> <li>query for simple word in the default texts fields (those having <code>full_text_search</code> property in your configuration)   by simply having the word in your query:   <pre><code>chocolate\n</code></pre>   Entries with text <code>chocolate</code></li> <li>or match exactly a full sentences by using quotes:   <pre><code>\"dark chocolate\"\n</code></pre>   Entries with text <code>dark chocolate</code></li> <li>you can also match a word or phrase in a specific field by using the field name, followed by a colon:   <pre><code>labels:organic\n</code></pre>   Entries with labels containing <code>organic</code></li> <li>you can have more than one term, the query will try to match all terms:   <pre><code>\"dark chocolate\" labels:organic\n</code></pre>   Entries with text <code>dark chocolate</code> and labels containing <code>organic</code></li> <li>you can combine queries with <code>AND</code>, <code>OR</code> and <code>NOT</code> operators, and use parenthesis to group them:   <pre><code>\"dark chocolate AND (labels:organic OR labels:vegan) AND NOT nutriscore:(e OR d)\"\n</code></pre>   Entries with text <code>dark chocolate</code>, labels containing <code>organic</code> or <code>vegan</code>, and Nutri-Score not <code>e</code> or <code>d</code></li> <li>you can query a sub field by using \".\" or \":\":   <pre><code>nutrients.sugar_100g:[10 TO 15]\n</code></pre>   equivalent to:   <pre><code>nutrients:sugar_100g:[10 TO 15]\n</code></pre>   Entries with sugar between 10 and 15 grams per 100g</li> <li>in range you can use * for unbounded values:   <pre><code>nutrients.sugar_100g:[* TO 20] AND nutrients.proteins_100g:[2 TO *]\n</code></pre>   Entries with sugar below 20 g and proteins above 2g for 100g</li> <li>match field existence with <code>*</code>:   <pre><code>nutriscore:*\n</code></pre>   Entries with Nutri-Score computed</li> </ul>"},{"location":"users/explain-query-language/#different-type-of-fields","title":"Different type of fields","text":"<p>When you created you configuration, you defined different fields types. It's important because the matching possibilities are not the same.</p> <p>In particular for text entries, there are two types of fields:</p> <ul> <li>keyword fields, that are used for exact matching</li> <li>full text fields, where part of the text can be matched, with complex matching possibilities</li> </ul> <p>There are also numeric and date fields, that can be used for range matching, or to make computations.</p>"},{"location":"users/explain-query-language/#full-text-queries","title":"Full text queries","text":"<p>FIXME add more on how we transform queries</p>"},{"location":"users/explain-taxonomies/","title":"Explain taxonomies","text":"<p>Taxonomies are a way to organize categorization of items.</p> <p>Normally, a taxonomy is about a specific field. For each possible values, it defines translations in different languages, and also possible synonyms (in each language). For each entry we have a canonical identifier.</p> <p>A taxonomy also organizes the entries within a direct acyclic graph (a hierarchy but with possibility of multiple parents, though always avoiding cycles). For example it may help describe that a salmon is a marine fish as well as a freshwater fish, and a oily fish.</p> <p>It can be used to help users find items using a specific field, in their language, even if they use a synonym for it.</p>"},{"location":"users/explain-taxonomies/#listing-taxonomies","title":"Listing taxonomies","text":"<p>If you plan to use taxonomies, you should first list them, in the taxonomy section of the configuration.</p> <p>Taxonomies must come in a JSON format, that can be downloaded at a particular URL.</p> <p>The data in the JSON must contain an object, where:</p> <ul> <li>each key correspond to the id of the taxonomy entries</li> <li>the value is an Object, with the following fields (none are mandatory):<ul> <li><code>name</code>: an Object associating language code,   with the entry name in the language (useful for translating the entry)</li> <li><code>synonyms</code>: an Object associating language code,   with an array of synonyms for this entry in this language</li> </ul> </li> </ul>"},{"location":"users/explain-taxonomies/#taxonomy-fields","title":"Taxonomy fields","text":"<p>As you define your fields in the configuration, you can specify that a field is a taxonomy field (<code>type: taxonomy</code>).</p> <p>In this case, you also have to provide the following fields:</p> <ul> <li>taxonomy_name: the name of the taxonomy (as defined in the configuration)</li> </ul> <ul> <li>synonyms_search: if true,   this will add a full text subfield that will enable using synonyms and translations to match this term.</li> </ul>"},{"location":"users/explain-taxonomies/#autocompletion-with-taxonomies","title":"Autocompletion with taxonomies","text":"<p>When you import taxonomies, they can be used to provide autocompletion in multiple ways.</p> <p>The webcomponents can use them to add values to facets, or to provide suggestions in the search bar.</p> <p>You can also use the autocompletion API</p>"},{"location":"users/explain-taxonomies/#importing-taxonomies","title":"Importing taxonomies","text":"<p>If you defined taxonomies, you must import them using the import-taxonomies command.</p>"},{"location":"users/explain-taxonomies/#technical-details-on-taxonomy-fields","title":"Technical details on taxonomy fields","text":"<p>A taxonomy field is stored in Elasticsearch as an object. For each language it has a specific field, but in this field we just store the taxonomy entry id (eg. for organic, we always store <code>en:organic</code>). The analyzer is almost set to <code>keyword</code> which means it won't be tokenized (but it is not completely true, as we also transform hyphen to underscore).</p> <p>Note that the value of this field must be considered a unique token by elasticsearch standard tokenizer. So you should only use letters, numbers, columns and the underscore. As an exception, we allow the hyphen character, transforming it to \"_\" before tokenization.</p> <p>But those field have a specific search analyzer, so that when you enter a search query, The query text is tokenized using standard analyzer, then lower cased, and we then look for synonyms in the taxonomy.</p>"},{"location":"users/how-to-install/","title":"How to install search-a-licious","text":""},{"location":"users/how-to-install/#prerequisites","title":"Prerequisites","text":""},{"location":"users/how-to-install/#ensure-mmap-count-is-high-enough","title":"Ensure mmap count is high enough","text":"<p>If you are on Linux, before running the services, you need to make sure that your system mmap count is high enough for Elasticsearch to run. You can do this by running:</p> <pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> <p>To make the change permanent, you need to add a line <code>vm.max_map_count=262144</code> to the <code>/etc/sysctl.conf</code> file and run the command <code>sudo sysctl -p</code> to apply the changes. This will ensure that the modified value of <code>vm.max_map_count</code> is retained even after a system reboot. Without this step, the value will be reset to its default value after a reboot.</p>"},{"location":"users/how-to-install/#install-docker-and-docker-compose","title":"Install docker and docker compose","text":"<p>search-a-licious uses docker and docker compose to manage the services it needs to run. You will need to install both of these before you can use search-a-licious.</p> <p>Once docker and docker compose are installed, clone the git repository locally.</p>"},{"location":"users/how-to-install/#settings","title":"Settings","text":"<p>All configuration are passed through environment variables to services through the use of a <code>.env</code> file. A sample <code>.env</code> file is provided in the repository, you will need to edit this file to suit your needs.</p> <p>The only required change is to set the <code>CONFIG_PATH</code> variable to the path of your YAML configuration file. This file is used to configure the search-a-licious indexer and search services. See the create your configuration, in tutorial</p> <p>If you want to see more about applications settings, see the Reference for Settings</p> <p>Look closely at each variable in the <code>.env</code> file. You must at the very least:</p> <ul> <li>change <code>RESTART_POLICY</code> to <code>always</code></li> <li>change <code>COMPOSE_FILE</code> to <code>docker-compose.yml;docker/prod.yml;docker/monitor.yml</code> (monitor is optional but recommended)</li> <li>change <code>MEM_LIMIT</code> to set elasticsearch memory limit</li> <li>change <code>NGINX_BASIC_AUTH_USER_PASSWD</code></li> </ul> <p>Then you can either:</p> <ul> <li>rebuild the docker images by running <code>make build</code></li> <li>use images from our github repository. For this,<ul> <li>edit the .env file and set <code>TAG</code> to the commit sha corresponding to the version you want to use</li> </ul> </li> </ul> <p>Our CI file for deployment might be of inspiration.</p>"},{"location":"users/how-to-install/#launching","title":"Launching","text":"<p>You should now be able to start docker:</p> <pre><code>docker compose up -d\n</code></pre> <p>[!NOTES] * You may encounter a permission error if your user is not part of the <code>docker</code> group, in which case you should either add it or modify the Makefile to prefix <code>sudo</code> to all docker and docker compose commands. * Update container might crash because if you are note connected to any Redis, Search-a-licious will still run. You need to connect to Redis only if you want continuous updates. See How to update the index</p>"},{"location":"users/how-to-install/#using-it","title":"Using it","text":"<p>To understand what you can then do, continue with the tutorial.</p>"},{"location":"users/how-to-update-index/","title":"How to update the index","text":"<p>As you use search-a-licious, you will first import the data, but then, you might need to update the index to keep it up to date with the latest data.</p> <p>There are two strategies to update the index:</p> <ul> <li>either you push events to the redis stream, and search-a-licious update the index continuously</li> <li>either you update the index from time to time using a new import of whole dataset</li> </ul>"},{"location":"users/how-to-update-index/#first-import","title":"First import","text":"<p>First import will populate Elasticsearch index with all the data.</p> <p>See initial import section in tutorial and <code>import</code> reference documentation</p> <p>It's important to note that if you don't use the continous updates strategy, you need to use <code>--skip-updates</code> option.</p>"},{"location":"users/how-to-update-index/#continuous-updates","title":"Continuous updates","text":"<p>To have continuous updates, you need to push events to the redis stream.</p> <p>Normally this will be done by your application.</p> <p>On each update/removal/etc. it must push events with at least:</p> <ul> <li>the document id</li> <li>and eventually more info (if you need them to filter out items, for example).</li> </ul> <p>Of course you can also imagine to push events from another service, if you have another way of getting changes, but this part is up to you.</p> <p>Then you just have to run the <code>updater</code> container that comes in the docker-compose configuration. <pre><code>docker-compose up -d updater\n</code></pre></p> <p>This will continuously fetch updates from the event stream, and update the index accordingly.</p> <p>At start it will compute last update time using the <code>last_modified_field_name</code> from the configuration to know from where to start processing the event stream.</p>"},{"location":"users/how-to-update-index/#updating-the-index-from-time-to-time-with-an-export","title":"Updating the index from time to time with an export","text":"<p>Another way to update the index is to periodically re-import the all the data, or changed data.</p> <p>This is less compelling to your users, but this might be the best way if you are using an external database publishing changes on a regular bases.</p> <p>For that you can use the <code>import</code> command, with the <code>--skip-updates</code> option, and with the <code>--partial</code> option if you are importing only changed data (otherwise it is advised to use the normal import process, which can be rolled-back (it create a new index)).</p>"},{"location":"users/how-to-update-index/#document-fetcher-and-pre-processing","title":"Document fetcher and pre-processing","text":"<p>In the configuration, you can define a <code>document_fetcher</code> and a <code>preprocessor</code> to transform the data.</p> <p>Those are fully qualified dotted names to python classes.</p> <p><code>document_fetcher</code> is only used on continuous updates, while <code>preprocessor</code> is used both on continuous updates and on initial import.</p>"},{"location":"users/how-to-use-scripts/","title":"How to use scripts","text":"<p>You can use scripts to sort results in your search requests.</p> <p>It enables to provides results that depends upon users defined preferences.</p> <p>This leverage a possibility of Elasticsearch of script based sorting.</p> <p>Using scripts needs the following steps:</p> <ol> <li>declare the scripts than can be used in the configuration</li> <li>import the scripts in Elasticsearch</li> <li>either use web-components to sort using scripts or call the search API using the script name, and providing parameters</li> </ol>"},{"location":"users/how-to-use-scripts/#declare-the-script-in-the-configuration","title":"Declare the script in the configuration","text":"<p>You have to declare the scripts that can be used for sorting in your configuration.</p> <p>This has two advantages:</p> <ul> <li>this keeps the API call simple, by just refering to the script by name</li> <li>this is more secure as you are in full control of scripts that are allowed to be used.</li> </ul> <p>The scripts section can look something like this: <pre><code>    scripts:\n      personal_score: # see 1\n        # see https://www.elastic.co/guide/en/elasticsearch/painless/8.14/index.html\n        lang: painless  # see 2\n        # the script source, here a trivial example\n        # see 3\n        source: |-\n          doc[params[\"preferred_field\"]].size &gt; 0 ? doc[params[\"preferred_field\"]].value : (doc[params[\"secondary_field\"]].size &gt; 0 ? doc[params[\"secondary_field\"]].value : 0)\n        # gives an example of parameters\n        # see 4\n        params:\n          preferred_field: \"field1\"\n          secondary_field: \"field2\"\n        # more non editable parameters, can be easier than to declare constants in the script\n        # see 5\n        static_params:\n          param1 : \"foo\"\n</code></pre></p> <p>Here:</p> <ol> <li> <p>we declare a script named <code>personal_score</code>, this is the name you will use in your API requests and/or web-components attributes</p> </li> <li> <p>we declare the language of the script, in this case <code>painless</code>, search-a-licious supports painless and Lucene expressions</p> </li> <li> <p>this is the source of the script. It can be a bit tedious to write those scripts. You can use the Elasticsearch documentation to get a better understanding of the language.</p> <p>In this example we are using a one liner, but your scripts can be far more complex.</p> </li> <li> <p>Parameters are a way to add inputs to the script.    You can declare them using an example. You can provide more complex structures, as allowed by JSON.    Those parameters will be given through the API requests</p> </li> <li> <p>static_params are parameters that are not allowed to change through the API.    It's mostly a way to declare constants in the script.    (hopefully more convenient than declaring them in the script)</p> </li> </ol> <p>For more information on configuration for scripts see configuration reference</p> <p>For informations on how to write scripts, see introduction to script in Elasticsearch documentation</p>"},{"location":"users/how-to-use-scripts/#import-the-scripts-in-elasticsearch","title":"Import the scripts in Elasticsearch","text":"<p>Each time you change the configuration, you have to import the scripts in Elasticsearch.</p> <p>For this you only need to run the sync-scripts command.</p> <pre><code>docker compose run --rm api python -m app sync-scripts\n</code></pre>"},{"location":"users/how-to-use-scripts/#using-web-components","title":"Using web components","text":"<p>After you have registered a script, it can be used for sorting using Search-a-licious provided web components.</p> <p>We imagine that you already have setup a search page, with, at least a <code>searchalicious-bar</code> (eventually refer to tutorial on building a search interface).</p> <p>In your <code>searchalicious-sort</code> component, you can add multiple sort options. While <code>searchalicious-sort-field</code> component add sorting on a field, you can use <code>searchalicious-sort-script</code> to add sorting on a script.</p> <p>This component has:</p> <ul> <li>an attribute to set the script name, corresponding to the name you have declared in the configuration.</li> <li>an attribute to set the parameters for this sort option.   This in turn can:<ul> <li>either be a string encoding a JSON Object (if your parameters are in some way static, or you set them through javascript)</li> <li>either be a key corresponding to a value in the local storage.   In this case it must be prefixed with <code>storage:</code>, and the value must be the key in the local storage.</li> </ul> </li> </ul> <p>Using static parameters can be an option if you are reusing the same script but for different scenarios. Imagine you have a script like the one given in example above, you could reuse the script to sort either on portion size or quantity (if no portion size), or to sort on nutriscore or sugar per 100g (if no nutriscore).</p> <p>Note that in this case you must provide an <code>id</code> to at least one of the sort option (because default id is based on script name).</p> <pre><code>&lt;searchalicious-sort&gt;\n  &lt;searchalicious-sort-script\n    script=\"personal_score\"\n    id=\"sort-by-quantity\"\n    parameters='{\"preferred_field\": \"portion_size\", \"secondary_field\": \"quantity\"}'\n  &gt;\n    Sort by portion size (fallback on quantity)\n  &lt;/searchalicious-sort-script&gt;\n  &lt;searchalicious-sort-script\n    script=\"personal_score\"\n    id=\"sort-by-nutrition\"\n    parameters='{\"preferred_field\": \"nutriscore\", \"secondary_field\": \"sugar_per_100g\"}'\n  &gt;\n    Sort by Nutri-Score (fallback on sugar per 100g)\n  &lt;/searchalicious-sort-script&gt;\n&lt;/searchalicious-sort&gt;\n</code></pre> <p>On the other side, using dynamic parameters can be an option if you want to let the user choose the field to sort on. For this you will need an independant way to set the values to sort on (your own UI) that either:</p> <ul> <li>dynamically modifies your searchalicious-sort-script element to change parameters property</li> <li>either stores it in local storage</li> </ul> <p>The later option as the advantage that it will survive a reload of the page or be still present on another visit. <pre><code>&lt;searchalicious-sort&gt;\n  &lt;searchalicious-sort-script\n    script=\"personal_score\"\n    parameters='local:personal-score-params'\n  &gt;\n    Sort according to my preferences\n  &lt;/searchalicious-sort-script&gt;\n&lt;/searchalicious-sort&gt;\n</code></pre></p>"},{"location":"users/how-to-use-scripts/#using-the-script-in-the-api","title":"Using the script in the API","text":"<p>You might also want to use the sort by script option in the API.</p> <p>For this:</p> <ul> <li>you must issue a POST request to the <code>/api/search</code> endpoint</li> <li>you must pass a JSON payload with:<ul> <li>the script name in the <code>sort_by</code> property</li> <li>you must provide the <code>sort_params</code>  property with a valid JSON object, corresponding to your parameters.</li> </ul> </li> </ul> <p>Let's use the same example as above, we could launch a search on whole database using our <code>personal_script</code> script, using curl <pre><code>curl -X POST -H \"Content-Type: application/json\" \\\n  -d '{\n    \"sort_by\": \"personal_score\",\n    \"sort_params\": {\"preferred_field\": \"nutriscore\", \"secondary_field\": \"sugar_per_100g\"}\n  }' \\\n  http://localhost:8000/api/search\n</code></pre></p>"},{"location":"users/how-to-use-scripts/#privacy-considerations","title":"Privacy considerations","text":"<p>The sort by script option was designed to allow users to sort their results according to their preferences.</p> <p>In the context of Open Food Facts, those preferences can reveal data which should remain privates.</p> <p>That's why we enforce using a <code>POST</code> request in the API (to avoid accidental logging), and we try hard not to log this data inside search-a-licious.</p>"},{"location":"users/how-to-use-scripts/#performance-considerations","title":"Performance considerations","text":"<p>When you use scripts for sorting, bare in mind that they needs to be executed on each document.</p> <p>Tests your results on your full dataset to make sure performances are not an issue.</p> <p>An heavy load on scripts sorting might affect other requests as well under an heavy load.</p>"},{"location":"users/ref-config/","title":"Reference for Configuration file","text":"<p>You can find the raw json schema here</p> <p>See configuration documentation on it's own page</p>"},{"location":"users/ref-openapi/","title":"Reference for Search-a-licious API","text":"<p>IMPORTANT: do not edit this file, it is replaced by OpenAPI documentation (using redocly at documentation generation time).</p>"},{"location":"users/ref-settings/","title":"Reference for Settings","text":"<p>You can find the raw json schema here</p> <p>See Settings documentation on it's own page</p>"},{"location":"users/ref-web-components/","title":"Reference for Search-a-licious Web Components","text":"<p>This page documents web Components provided by Search-a-licious to quickly build your interfaces.</p> <p>See the tutorial for an introduction</p>"},{"location":"users/ref-web-components/#customization","title":"Customization","text":""},{"location":"users/ref-web-components/#styling","title":"Styling","text":"<p>We added a lot of <code>part</code> attributes to the components, to allow you to customize the look and feel of the components. See ::part() attribute documentation on MDN</p>"},{"location":"users/ref-web-components/#translations","title":"Translations","text":"<p>We only translated basic messages and most labels can generally be overridden using slots inside web component, where your own translation framework might be use (be it in javascript, or through your template engine or any technique).</p> <p>If you however needs to override current translations, you might clone this project, change translations in xliff files and regenerate the bundle.</p>"},{"location":"users/ref-web-components/#main-components","title":"Main components","text":"<p>Those are the components you will certainly use to build your interface. Of course none are mandatory and you can pick and choose the components you need.</p> <p>Bare attention to the <code>search-name</code> attribute which must correspond to the <code>name</code> attribute of the search bar. If you do not specify it, it will be the default one. You'll need it if you mix multiple search bars in the same page.</p>"},{"location":"users/ref-web-components/#searchalicious-bar","title":"searchalicious-bar","text":""},{"location":"users/ref-web-components/#searchalicious-results","title":"searchalicious-results","text":""},{"location":"users/ref-web-components/#searchalicious-pages","title":"searchalicious-pages","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-taxonomy-suggest","title":"searchalicious-taxonomy-suggest","text":""},{"location":"users/ref-web-components/#searchalicious-facets","title":"searchalicious-facets","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-button","title":"searchalicious-button","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-count","title":"searchalicious-count","text":"<p>"},{"location":"users/ref-web-components/#sorting","title":"Sorting","text":""},{"location":"users/ref-web-components/#searchalicious-sort","title":"searchalicious-sort","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-sort-field","title":"searchalicious-sort-field","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-sort-script","title":"searchalicious-sort-script","text":"<p>"},{"location":"users/ref-web-components/#charts-components","title":"Charts components","text":"<p>Charts components are based on vega.</p>"},{"location":"users/ref-web-components/#searchalicious-distribution-chart","title":"searchalicious-distribution-chart","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-scatter-chart","title":"searchalicious-scatter-chart","text":"<p>"},{"location":"users/ref-web-components/#layout-components","title":"Layout components","text":"<p>Layout widgets are used to layout the page, they are not mandatory but can be useful. It must not create dependencies with other components.</p>"},{"location":"users/ref-web-components/#searchalicious-panel-manager","title":"searchalicious-panel-manager","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-layout-page","title":"searchalicious-layout-page","text":"<p>"},{"location":"users/ref-web-components/#internal-components","title":"Internal components","text":"<p>Those are components that are not intended to be used directly by the user, but are used internally by the other components.</p>"},{"location":"users/ref-web-components/#searchalicious-facet-terms","title":"searchalicious-facet-terms","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-suggestion-entry","title":"searchalicious-suggestion-entry","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-checkbox","title":"searchalicious-checkbox","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-radio","title":"searchalicious-radio","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-toggle","title":"searchalicious-toggle","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-secondary-button","title":"searchalicious-secondary-button","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-button-transparent","title":"searchalicious-button-transparent","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-icon-cross","title":"searchalicious-icon-cross","text":"<p>"},{"location":"users/ref-web-components/#searchalicious-suggestion-entry_1","title":"searchalicious-suggestion-entry","text":"<p>"},{"location":"users/tutorial/","title":"Tutorial - Using search-a-licious in your project","text":"<p>So you have a dataset, or a project, with a collection of data you want to make searchable. Search-a-licious can help you have it done in a matter of few hours, while retaining your choices.</p> <p>In this tutorial, we will see how we can use search-a-licious to search Open Food Facts data.</p>"},{"location":"users/tutorial/#setting-up","title":"Setting up","text":""},{"location":"users/tutorial/#clone-the-repository","title":"Clone the repository","text":"<p>As an easy way to setup the project we will clone the repository:</p> <pre><code>git clone git@github.com:openfoodfacts/search-a-licious.git\ncd search-a-licious\n</code></pre>"},{"location":"users/tutorial/#create-a-configuration-file","title":"Create a configuration file","text":"<p>We need to create a configuration file to indicate which fields we care about in our index.</p> <p>For this we can create a conf/data/openfoodfacts-tutorial.yml file. It uses the YAML format.</p>"},{"location":"users/tutorial/#default-index-and-indices","title":"Default Index and Indices","text":"<p>At the top we have:</p> <pre><code>default_index: \"off\" # see 2\nindices:  # see 1\n  off:    # see 2\n</code></pre> <ol> <li> <p>Search-a-licious support serving more than one dataset at once,   each dataset as it's own indice with it's own definition.   So we start with the \"indices\" keyword.</p> </li> <li> <p>We must give a name to our unique index. Let's call it <code>off</code> as Open Food Facts.   We also indicate that this is the default index for the API.</p> </li> </ol>"},{"location":"users/tutorial/#index-configuration","title":"Index Configuration","text":"<p>Now comes important indications for the index: <pre><code>...\nindices:\n  off:\n    index:  # see 1\n      id_field_name: code  # see 2\n      last_modified_field_name: last_modified_t  # see 2\n      name: openfoodfacts  # see 3\n</code></pre></p> <ol> <li>The index section mark settings that are specific to the index.</li> <li>We have to then indicate two important fields:<ul> <li>a field that contains a unique id for each item in the collection (the identifier).    In Open Food Facts dataset, it's simply the barcode, stored in <code>code</code><ul> <li>a field that contains the item modification date to enable incremental updates.   In Open Food Facts dataset, it's <code>last_modified_t</code></li> </ul> </li> </ul> </li> <li>We have to give a sensible name to the index, which should be unique for our ElasticSearch instance.    So we simply put \"openfoodfacts\".</li> </ol>"},{"location":"users/tutorial/#configuring-searchable-fields","title":"Configuring Searchable Fields","text":"<p>Let's continue with configuration of fields we want to be able to search.</p> <pre><code>...\nindices:\n  off:\n    ...\n    fields:  # see 1\n      code:  # see 2\n        required: true\n        type: keyword\n      product_name:  # see 3\n        full_text_search: true\n        type: text_lang\n      labels_tags:  # see 4\n        type: keyword\n        taxonomy_name: label\n        bucket_agg: true\n      labels:  # see 5\n        full_text_search: true\n        input_field: labels_tags\n        taxonomy_name: label\n        type: taxonomy\n      nutriscore_grade:  # see 6\n        type: keyword\n        bucket_agg: true\n      last_modified_t:  # see 7\n        type: date\n</code></pre> <ol> <li>now we are in the field section</li> <li>code is our identifier, it contains the barcode<ul> <li>we mark it required for we should reject elements without it (it's certainly a bug)</li> <li>Each field must have a type which indicates how to handle it.</li> <li>For the code we choose the type <code>keyword</code> which means it's a fixed token.</li> </ul> </li> <li>product_name is also an important field.<ul> <li>this time we want to be able to search part of it,   also it's a field that comes with different values for different languages,   that's why we mark it as <code>type: textlang</code></li> <li>We also include this field in free text search,   that's the <code>full_text_search: true</code> part.</li> </ul> </li> <li><code>labels_tags</code> is a field that contains labels of the product in a canonical form.<ul> <li>This is a field supported by a taxonomy.   Taxonomies gives translations and synonyms for terms of a specific field of interest.   In this case this is the <code>label</code> taxonomy. We will see later on the configuration of taxonomies.   This field will be useful to find all items matching a particular label in a precise way,   using the label canonical form.</li> <li>Also we plan to use the field for facets so we put the <code>bucket_agg: true</code> part.</li> </ul> </li> <li>The labels repeat information from <code>labels_tags</code>    (hence the <code>input_field: labels_tags</code>)    but is there to enrich the information of full text searches    (<code>type: taxonomy</code>).    With that, the full text search will take into account the translations and synonyms of labels.</li> <li>Nutri-Score is a field with very basic values: \"a\", \"b\", ..., \"e\", \"unknown\" and \"not-applicable\",   as for code, a perfect case for a <code>keyword</code> field</li> <li>last_modified_t, corresponding to last modification time, is an example of a date field.</li> </ol>"},{"location":"users/tutorial/#configuring-taxonomies","title":"Configuring Taxonomies","text":"<p>Let's continue with configuration of taxonomies.</p> <p>Taxonomies will be used in multiple ways by search-a-licious:</p> <ul> <li>to add synonyms for taxonomized values on full text search.   For example you might search for \"European Organic\",   to find items with the \"EU Organic\" label.</li> <li>to suggest values to search for in autocomplete (this can also be used in edit forms)</li> <li>to translate values from the results, for example as we display facets</li> </ul> <pre><code>...\nindices:\n  off:\n    ...\n    taxonomy:\n      sources:  # see 1\n\n      - name: label\n        url: https://static.openfoodfacts.org/data/taxonomies/labels.full.json\n      exported_langs:   # see 2\n      - en\n      - fr\n      index:   # see 3\n        name: off_taxonomy\n</code></pre> <ol> <li>We cited one taxonomy above: <code>label</code>, here we define how to get it.</li> <li>We must also defined which languages we want to use for this taxonomy.    There is some trade-off between having a manageable size for the index and supporting more languages.</li> <li>finally we have to give a name to the Elasticsearch index that will contain the taxonomies.</li> </ol>"},{"location":"users/tutorial/#configuring-supported-languages","title":"Configuring Supported Languages","text":"<p>We continue with languages configurations:</p> <pre><code>...\n...\nindices:\n  off:\n    ...\n    supported_langs: [\"en\", \"fr\"]\n    lang_separator: \"_\"\n</code></pre> <ol> <li>the list of supported languages tells which languages will be retained in our index</li> <li>the \"lang separator\" helps us tells that, the fields are suffixed by the language using this separator.   In our case it means, for example, that <code>product_name_fr</code> contains the french version of <code>product_name</code>.</li> </ol> <p>We have our configuration ready. That was a bit though, but this was the hardest part !</p> <p>Don't hesitate to read the Reference for Configuration file to learn more.</p>"},{"location":"users/tutorial/#setup-the-project","title":"Setup the project","text":"<p>In the project you can modify the <code>.env</code> file and change variables you need to change, but for now, the only mandatory variable to change is the one that will point to our configuration file.</p> <pre><code>...\n# Path to the yaml configuration file\n# This envvar is **required**\nCONFIG_PATH=`data/config/openfoodfacts.yml`\n</code></pre> <p>See the Reference for Settings to learn about other settings.</p>"},{"location":"users/tutorial/#initial-import","title":"Initial import","text":""},{"location":"users/tutorial/#getting-the-data","title":"Getting the data","text":"<p>Now that it's all done, we are ready to start to import the data.</p> <p>First we start the Elasticsearch index, we will also start the ElasticVue service to be able to look at what happens: <pre><code>docker compose up -d es01 es02 elasticvue\n</code></pre></p> <p>There is an export of all the openfoodfacts data in JSONL available on at  https://static.openfoodfacts.org/data . But it's a very big file !  For this tutorial we will prefer to use a sample of products at https://static.openfoodfacts.org/data/exports/products.random-modulo-10000.jsonl.gz</p> <p>Put this file in the data/ directory which is bind mounted in the container. On linux we can do it with: <pre><code>wget https://static.openfoodfacts.org/data/exports/products.random-modulo-10000.jsonl.gz -O data/products.random-modulo-10000.jsonl.gz\n</code></pre></p>"},{"location":"users/tutorial/#import-the-data","title":"Import the data","text":"<p>We will then import this file in our index, we have a specific command for that:</p> <pre><code>docker compose run --rm api python3 -m app import /opt/search/data/products.random-modulo-10000.jsonl --skip-updates\n</code></pre> <p>The first part is simply to run a command using our docker container. The <code>python3 -m app import</code> part is to run the <code>import</code> command provided by our <code>app</code> module.</p> <p>We use the <code>--skip-updates</code> flag because we don't have a redis stream to connect to, to look for updates.</p> <p>We also need our taxonomy, and there is a command <code>import-taxonomies</code> to get it.</p> <pre><code>docker compose run --rm api python3 -m app import-taxonomies\n</code></pre> <p>You can read more about this process reading How to update index documentation.</p>"},{"location":"users/tutorial/#inspecting-elasticsearch","title":"Inspecting Elasticsearch","text":"<p>We can take a look at what just happened by using ElasticVue, a simple but handy tool to inspect Elasticsearch.</p> <p>Open http://127.0.0.1:8080 in your browser.</p> <p>If it's the fist time, click \"ADD ELASTICSEARCH CLUSTER\" and use \"No authorization\", cluster name: docker-cluster, uri: http://localhost:9200</p> <p>Click on the button which says there are 3 indices.</p> <p>You shall see two indices:</p> <ul> <li>one named <code>openfoodfacts-&lt;date of initial import&gt;</code> with alias <code>openfoodfacts</code></li> <li>one named <code>off_taxonomy-&lt;date of initial import&gt;</code> with alias <code>off_taxonomy</code> As you already guessed, the first contains our food products, and the second our taxonomies.</li> </ul> <p></p> <p>The \"Lucene docs\" column gives you an idea of the number of entries you have in each index.</p> <p>You can click on an index to view it's content and have a feeling of what we just imported.</p>"},{"location":"users/tutorial/#using-the-search-api","title":"Using the search API","text":""},{"location":"users/tutorial/#starting-the-service","title":"Starting the service","text":"<p>We don't have an interface to search at the moment, but we can use the API.</p> <p>It would be perfectly ok to only deploy the interface, maybe because you will call it from your own application either to provide search to your users, or to implement a very specific feature which is based upon a search request.</p> <p>Let's start our search-a-licious service:</p> <pre><code>docker compose up  es01 es02 api frontend\n</code></pre>"},{"location":"users/tutorial/#running-our-first-query","title":"Running our first query","text":"<p>We start the <code>api</code> container, which is the search-a-licious backend, and the frontend as it is a nginx acting as a reverse proxy.</p> <p>Now let's open http://127.0.0.1:8000/docs</p> <p>You can see the documentation of the various API offered by search-a-licious service.</p> <p>Let's concentrate on the GET <code>/search</code> service. We can test it using the Try it out button.</p> <p>We can try a simple search of fair trade in the q parameter, we get 17 results.</p> <p>Interesting fields in the JSON we receive includes:</p> <ul> <li><code>hits</code> where we have the detail of each result.   In each results we retrieve full information about an item, that's a lot of data.   We might optimize this using the <code>fields</code> query parameter.</li> <li><code>page</code>: the current returned page, <code>page_count</code> the number of pages, and <code>page_size</code> the number of results per page.</li> <li><code>count</code> is the total number of items returned.   <code>is_count_exact</code>, when false indicate that for performance reason, we did not compute the total number of results,   but there are at least <code>count</code> results.</li> </ul>"},{"location":"users/tutorial/#sorting-results","title":"Sorting results","text":"<p>As you get the results you might want them according to a particular order. Say we search for fair trade and we want the results to be sorted by the nutriscore grade.</p> <p>We simply repeat the above query with <code>q</code> = <code>fair trade</code> and <code>sort_by</code> = <code>nutriscore_grade</code>.</p>"},{"location":"users/tutorial/#limiting-fields","title":"Limiting fields","text":"<p>In the previous example, we might only be interested in the name of the product and it's Nutri-Score. We can limit the fields return by using the <code>fields</code> parameter. Here it would be <code>product_name,nutriscore_grade</code></p>"},{"location":"users/tutorial/#using-filters","title":"Using filters","text":"<p>We may want to be more precise on our request. Now let's ask products which really have \"fair-trade\" label.</p> <p>For this we will use our query field in a more advanced way.</p> <p>We can use this value for the \"q\" field: <code>labels_tags:\"en:fair-trade\"</code>. Using this we specify that we search for the key \"en:fair-trade\" in the value.<sup>1</sup></p> <p>We may then want to restrict our search to fair trade products that also have the \"EU organic\" label. For this we can use <code>labels_tags:(\"en:fair-trade\" AND \"en:eu-organic\")</code>.</p> <p>If we wanted products having one or the other <code>labels_tags:(\"en:fair-trade\" OR \"en:eu-organic\")</code> would do.</p> <p>We can also combine those filters with a search. Using <code>cocoa labels_tags:\"en:fair-trade\"</code> will help find some fair trade cocoa.</p> <p>You can find more about the search query syntax in Explain Query Language</p>"},{"location":"users/tutorial/#getting-facets","title":"Getting facets","text":"<p>Remember in our configuration we added a <code>bucket_agg: true</code> on some fields. This will enable us to get facets on those fields.</p> <p>Let's go back to our API, and use <code>cocoa labels_tags:\"en:fair-trade\"</code> in the request, and ask for facet <code>nutriscore_grade</code>. We get a result with same fields as for previous searches, but we have a new <code>facets</code> fields.</p> <p>It returns something like <pre><code>  \"facets\": {\n    \"nutriscore_grade\": {\n      \"name\": \"nutriscore_grade\",\n      \"items\": [\n        {\n          \"key\": \"e\",\n          \"name\": \"e\",\n          \"count\": 3,\n          \"selected\": false\n        },\n        {\n          \"key\": \"d\",\n          \"name\": \"d\",\n          \"count\": 2,\n          \"selected\": false\n        }\n      ],\n      \"count_error_margin\": 0\n    }\n  }\n</code></pre></p> <p>As you can see we have the \"nutriscore_grade\" facet with two value: \"e\" and \"d\", and we got the document count for each value.</p> <p>The <code>selected</code> field was deduced from an analysis of the request. If we were to ask for the <code>labels_tags</code> facets with the same request, the <code>en:fair-trade</code> label would be selected, as it is an active filter. (note that it only works if you write the query in a specific way).</p> <p>You can try and ask both facets by using <code>nutriscore_grade,labels_tags</code> as the facets parameter.</p>"},{"location":"users/tutorial/#creating-a-search-page-using-web-components","title":"Creating a search Page using web components","text":"<p>Now that we have played around with the API, we might want to create a search page to let users do their own search in a visual way.</p> <p>That's were Search-a-licious also got you covered.</p> <p>Search-a-licious provides web components that let you build your search page easily.</p>"},{"location":"users/tutorial/#building-a-search-interface","title":"Building a search interface","text":"<p>Let's try it and create a static html page. We won't care much about the \"look and feel\" for now, for sake of simplicity.</p> <p>A simple way to serve the page, is to add a file in the <code>frontend/public</code> folder<sup>2</sup></p> <p>Let's create a <code>tutorial.html</code> file with a very basic initial content:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Search-a-licious tutorial&lt;/title&gt;\n    &lt;!-- this makes the various searchalicious components available --&gt;\n    &lt;script type=\"module\" src=\"./search-a-licious.bundled.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div id=\"search-bar\"&gt;\n  &lt;!-- the search bar where you can add text --&gt;\n  &lt;searchalicious-bar&gt;&lt;/searchalicious-bar&gt;\n  &lt;!-- the button to launch the search --&gt;\n  &lt;searchalicious-button&gt;&lt;/searchalicious-button&gt;\n  &lt;!-- sorting --&gt;\n  &lt;searchalicious-sort auto-refresh&gt;\n    &lt;searchalicious-sort-field field=\"nutriscore_grade\"&gt;Best nutriscore&lt;/searchalicious-sort-field&gt;\n    &lt;searchalicious-sort-field field=\"-nutriscore_grade\"&gt;Worst nutriscore&lt;/searchalicious-sort-field&gt;\n  &lt;/searchalicious-sort&gt;\n  &lt;/div&gt;\n  &lt;div id=\"results\"&gt;\n  &lt;!-- display of results --&gt;\n  &lt;searchalicious-results&gt;\n    &lt;!-- this define a template for results. We can use ${} expression with a result object containing result fields --&gt;\n    &lt;template slot=\"result\"&gt;\n      &lt;li&gt;\n        &lt;a href=\"https://world.openfoodfacts.org/${result.code}\"&gt;${result.product_name}&lt;/a&gt;\n        \u2212 Nutri-Score: ${result.nutriscore_grade}\n      &lt;/li&gt;\n    &lt;/template&gt;\n  &lt;/searchalicious-results&gt;\n  &lt;!-- a small display of the number of results --&gt;\n  &lt;searchalicious-count&gt;&lt;/searchalicious-count&gt;\n  &lt;!-- pagination --&gt;\n  &lt;searchalicious-pages&gt;&lt;/searchalicious-pages&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>As you can see this is a big file but not so big for search page.</p> <p>If you know html you should be able to read this quite easily. Let's focus on some elements:</p> <ul> <li>first of all we have to import the search-a-licious library to make it available.   This is done through the classical <code>script</code> element (in <code>head</code>),   but note the <code>type=\"module\"</code> property.</li> <li>next we have the <code>searchalicious-bar</code> component. This is the central component.   Not only will responsible for the input in the search bar,   but it also centralize the search options.</li> <li>then we have the <code>searchalicious-button</code>, which, as you guessed, is just responsible to launch the search   (hitting enter in the search bar also works, but it might be more evident for some users with a button)</li> <li>Sorting is a bit more complex, we have the <code>searchalicious-sort</code> component,   and inside we added the options we want to offer for sorting.   The <code>field</code> property takes the same value as you would put in the <code>sort_by</code> parameter of the search API,   and the inner text will be displayed to the user.</li> <li>To display our results, we need the <code>serachalicious-results</code> component.   Inside it we need to define a template, with the property <code>slot</code> set to <code>result</code>.   In this template we can use simple variable interpolation   as in a javascript template literal,   with the <code>result</code> variable containing one search result, as returned by the API.   This will be used to display each results.   As the results use a <code>&lt;ul&gt;</code> element, it's better to enclose your result in a <code>&lt;li&gt;</code>.</li> <li>The <code>searchalicious-count</code> component will simply display the number of results.</li> <li>Finally the <code>searchalicious-pages</code> component is there to display a list of pages.</li> </ul>"},{"location":"users/tutorial/#trying-our-interface","title":"Trying our interface","text":"<p>Let's try to use it !</p> <p>We can go to <code>http://localhost:8000/static/tutorial.html</code> and see our interface.</p> <p>FIXME screen capture</p> <p>We can type <code>fair trade</code> in the search bar, hit the search buttons and see results displayed.</p> <p>FIXME screen capture</p> <p>We can use the pages navigation to browse all results.</p> <p>We can also type a more advanced search like <code>cocoa labels_tags:\"en:fair-trade\"</code></p> <p>You can also test that sorting by best or worst nutriscore is working just fine !</p> <p>If you are curious, you can open the dev toolbar (F12),  go in the network tab, select to only view XHR requests (requests sent by javascript), and see how each search is using the search API.</p> <ol> <li> <p>Note that we have to use \"\" around value here, because the value contains a \":\" inside.<code>labels_tags:en:fair-trade</code>  would be interpreted as asking for a field named labels_tags.en  having the value fair-trade.\u00a0\u21a9</p> </li> <li> <p>this only works right away if you are using the service in developer mode, which should be the case if you followed this tutorial. Of course in production, this might be served by your own servers.\u00a0\u21a9</p> </li> </ol>"}]}